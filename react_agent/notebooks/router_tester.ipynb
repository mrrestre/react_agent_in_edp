{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util.router import Router\n",
    "from testdata.email import EMAIL\n",
    "\n",
    "router = Router()\n",
    "output = router.route_input(EMAIL)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from gen_ai_hub.proxy.langchain import init_llm\n",
    "from pydantic import BaseModel, Field\n",
    "from testdata.email import EMAIL\n",
    "\n",
    "\n",
    "class RouterOutputModel(BaseModel):\n",
    "    \"\"\"Analyze the unread email and route it according to its content.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(...,\n",
    "        description=\"Step-by-step reasoning behind the classification.\"\n",
    "    )\n",
    "    classification: Literal[\"ignore\", \"respond\", \"notify\"] = Field(...,\n",
    "        description=\"The classification of an email: 'ignore' for irrelevant emails, \"\n",
    "        \"'notify' for important information that doesn't need a response, \"\n",
    "        \"'respond' for emails that need a reply\",\n",
    "    )\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(..., description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(\n",
    "        ..., description=\"Why this query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "llm = init_llm(\"gpt-4o\")\n",
    "if llm is None:\n",
    "    raise ValueError(\"LLM initialization failed! Check API key or `gen_ai_hub` installation.\")\n",
    "\n",
    "print(f\"LLM initialized: {llm}\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "print(f\"Structured LLM: {structured_llm}\")\n",
    "\n",
    "email_text = EMAIL.get(\"body\", \"No email content provided.\")\n",
    "print(f\"Email Text: {email_text}\")  # Debugging\n",
    "\n",
    "prompt = f\"Analyze the following email and classify it:\\n\\n{email_text}\"\n",
    "\n",
    "try:\n",
    "    output = structured_llm.invoke(prompt)\n",
    "    print(f\"Model Output: {output}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error invoking structured_llm: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No deployment found with: deployment.model_name == gemini-1.5-pro",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgen_ai_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproxy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#llm = ChatOpenAI(proxy_model_name='gpt-4o', temperature=0)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m llm = \u001b[43minit_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgemini-1.5-pro\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(llm.invoke(\u001b[33m\"\u001b[39m\u001b[33mAre you there?\u001b[39m\u001b[33m\"\u001b[39m).content)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSearchQuery\u001b[39;00m(BaseModel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/gen_ai_hub/proxy/langchain/init_models.py:237\u001b[39m, in \u001b[36minit_llm\u001b[39m\u001b[34m(proxy_client, temperature, max_tokens, top_k, top_p, init_func, model_id, *args, **kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_id:\n\u001b[32m    236\u001b[39m     model_kwargs[\u001b[33m'\u001b[39m\u001b[33mmodel_id\u001b[39m\u001b[33m'\u001b[39m] = model_id\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mModelType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m                   \u001b[49m\u001b[43minit_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/gen_ai_hub/proxy/langchain/init_models.py:190\u001b[39m, in \u001b[36m_init_model\u001b[39m\u001b[34m(proxy_client, model_type, args, kwargs, init_func, model_kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m init_func:\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _init_custom_model(proxy_client=proxy_client, init_func=init_func, args=args, kwargs=kwargs,\n\u001b[32m    189\u001b[39m                               model_kwargs=model_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m retrieval_result: RetrievalResult = \u001b[43mcatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m                                                     \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retrieval_result.registry_entry.init_func(proxy_client=retrieval_result.proxy_client,\n\u001b[32m    195\u001b[39m                                                  deployment=retrieval_result.deployment,\n\u001b[32m    196\u001b[39m                                                  **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/gen_ai_hub/proxy/langchain/init_models.py:134\u001b[39m, in \u001b[36mCatalog.retrieve\u001b[39m\u001b[34m(self, proxy_client, args, kwargs, model_type)\u001b[39m\n\u001b[32m    128\u001b[39m registry_entry = \u001b[38;5;28mself\u001b[39m._get_registry_entry(\n\u001b[32m    129\u001b[39m     proxy_client=proxy_client,\n\u001b[32m    130\u001b[39m     model_name=model_name,\n\u001b[32m    131\u001b[39m     model_type=model_type\n\u001b[32m    132\u001b[39m )\n\u001b[32m    133\u001b[39m f_select_deployment = registry_entry.f_select_deployment \u001b[38;5;129;01mor\u001b[39;00m default_f_select_deployment\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m deployment = \u001b[43mf_select_deployment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_identification_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m RetrievalResult(proxy_client=proxy_client, deployment=deployment, registry_entry=registry_entry)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/gen_ai_hub/proxy/langchain/init_models.py:33\u001b[39m, in \u001b[36mdefault_f_select_deployment\u001b[39m\u001b[34m(proxy_client, **model_identification_kwargs)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_f_select_deployment\u001b[39m(proxy_client: BaseProxyClient,\n\u001b[32m     32\u001b[39m                                 **model_identification_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) -> BaseDeployment:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproxy_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_deployment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_identification_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/gen_ai_hub/proxy/gen_ai_hub_proxy/client.py:232\u001b[39m, in \u001b[36mGenAIHubProxyClient.select_deployment\u001b[39m\u001b[34m(self, raise_on_multiple, **search_key_value)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m matched_deployments[\u001b[32m0\u001b[39m]\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mNo deployment found with: \u001b[39m\u001b[33m'\u001b[39m + \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\n\u001b[32m    233\u001b[39m         [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdeployment.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m == \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m search_key_value.items()]\n\u001b[32m    234\u001b[39m     ))\n",
      "\u001b[31mValueError\u001b[39m: No deployment found with: deployment.model_name == gemini-1.5-pro"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from gen_ai_hub.proxy.langchain import init_llm\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "#llm = ChatOpenAI(proxy_model_name='gpt-4o', temperature=0)\n",
    "llm = init_llm('gemini-1.5-pro', max_tokens=1024)\n",
    "print(llm.invoke(\"Are you there?\").content)\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(..., description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(\n",
    "        ..., description=\"Why this query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "# Invoke the augmented LLM\n",
    "print(structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\").content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "react-agent-CUxtcVGL-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
