{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f679303",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'beta'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m structured_llm = llm.with_structured_output(schema)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Invoke the augmented LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m msg = \u001b[43mstructured_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHow does Calcium CT score relate to high cholesterol?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m msg.tool_calls\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/langchain_core/runnables/base.py:3023\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3021\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3022\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3023\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3024\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3025\u001b[39m         \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/langchain_core/runnables/base.py:5358\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5353\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5354\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[32m   5355\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5356\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5357\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5359\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5360\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5361\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5362\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:307\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    298\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m     **kwargs: Any,\n\u001b[32m    303\u001b[39m ) -> BaseMessage:\n\u001b[32m    304\u001b[39m     config = ensure_config(config)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    306\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    317\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:843\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    836\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    837\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    840\u001b[39m     **kwargs: Any,\n\u001b[32m    841\u001b[39m ) -> LLMResult:\n\u001b[32m    842\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:683\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    682\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         )\n\u001b[32m    690\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    691\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:908\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:901\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    899\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m.chat.completions.parse(**payload)\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    903\u001b[39m     _handle_openai_bad_request(e)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'beta'"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from gen_ai_hub.proxy.langchain import init_llm\n",
    "from gen_ai_hub.proxy.gen_ai_hub_proxy import GenAIHubProxyClient\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(description=\"Why this query is relevant to the user's request.\")\n",
    "    \n",
    "schema = {\n",
    "    \"title\": \"Output\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"search_query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Query that is optimized web search.\",\n",
    "        },\n",
    "        \"justification\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Why this query is relevant to the user's request.\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(proxy_model_name='gpt-4o', temperature=0)\n",
    "llm = init_llm(proxy_client=GenAIHubProxyClient(), model_name='gpt-4o', max_tokens=1024)\n",
    "#llm = init_llm('gemini-1.5-pro', max_tokens=100)\n",
    "# print(llm.invoke(\"Are you there?\").content)\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(schema)\n",
    "\n",
    "# Invoke the augmented LLM\n",
    "msg = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
    "\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal, TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# Schema for structured output to use as routing logic\n",
    "class Route(BaseModel):\n",
    "    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n",
    "        None, description=\"The next step in the routing process\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "router = llm.with_structured_output(Route)\n",
    "\n",
    "\n",
    "# State\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call_1(state: State):\n",
    "    \"\"\"Write a story\"\"\"\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_2(state: State):\n",
    "    \"\"\"Write a joke\"\"\"\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_3(state: State):\n",
    "    \"\"\"Write a poem\"\"\"\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_router(state: State):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "    # Run the augmented LLM with structured output to serve as routing logic\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Route the input to story, joke, or poem based on the user's request.\"\n",
    "            ),\n",
    "            HumanMessage(content=state[\"input\"]),\n",
    "        ]\n",
    "    )\n",
    "    return {\"decision\": decision.step}\n",
    "\n",
    "\n",
    "# Conditional edge function to route to the appropriate node\n",
    "def route_decision(state: State):\n",
    "    # Return the node name you want to visit next\n",
    "    if state[\"decision\"] == \"story\":\n",
    "        return \"llm_call_1\"\n",
    "    elif state[\"decision\"] == \"joke\":\n",
    "        return \"llm_call_2\"\n",
    "    elif state[\"decision\"] == \"poem\":\n",
    "        return \"llm_call_3\"\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "router_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "router_builder.add_node(\"llm_call_1\", llm_call_1)\n",
    "router_builder.add_node(\"llm_call_2\", llm_call_2)\n",
    "router_builder.add_node(\"llm_call_3\", llm_call_3)\n",
    "router_builder.add_node(\"llm_call_router\", llm_call_router)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "router_builder.add_edge(START, \"llm_call_router\")\n",
    "router_builder.add_conditional_edges(\n",
    "    \"llm_call_router\",\n",
    "    route_decision,\n",
    "    {  # Name returned by route_decision : Name of next node to visit\n",
    "        \"llm_call_1\": \"llm_call_1\",\n",
    "        \"llm_call_2\": \"llm_call_2\",\n",
    "        \"llm_call_3\": \"llm_call_3\",\n",
    "    },\n",
    ")\n",
    "router_builder.add_edge(\"llm_call_1\", END)\n",
    "router_builder.add_edge(\"llm_call_2\", END)\n",
    "router_builder.add_edge(\"llm_call_3\", END)\n",
    "\n",
    "# Compile workflow\n",
    "router_workflow = router_builder.compile()\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(router_workflow.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "state = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\n",
    "print(state[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "react-agent-CUxtcVGL-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
