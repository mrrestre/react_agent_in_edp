{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "from react_agent.src.agents.react_agent import ReActAgent\n",
    "from react_agent.src.agents.triage import Triage\n",
    "\n",
    "from react_agent.src.util.tools_fabric import ToolsFabric\n",
    "\n",
    "from experiments.models.experiment_models import LabeledQAPairFacts, ExperimentResult\n",
    "from experiments.fact_score.fact_scorer import FactScorer\n",
    "from experiments.metrics.bert_score import BertScore\n",
    "from experiments.metrics.llm_judge import LLMAsJudgeEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MCP = False\n",
    "DEBUG_MODE = False\n",
    "\n",
    "LLM_JUDGE_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "fact_scorer = FactScorer()\n",
    "path_to_ressources = \"./resources/02_facts/\"\n",
    "\n",
    "files = {\n",
    "    \"extensibility\": \"extensibility_assistance_facts.json\",\n",
    "    \"malaysia\": \"malaysia_support_facts.json\",\n",
    "    \"peppol\": \"peppol_support_facts.json\",\n",
    "    \"all\": \"all_cases_facts.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.abspath(path_to_ressources + files[\"all\"])\n",
    "\n",
    "\n",
    "with open(file_path, encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "    data_set = [LabeledQAPairFacts(**item) for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 3 random elements\n",
    "random_data_set = random.sample(data_set, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db41a0d424884c1da56cccf0dbce26e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running experiments:   0%|          | 0/1 [00:00<?, ?experiment/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking agent question with id: EA-17\n",
      "Finished agent execution\n",
      "Started FactScore calculation\n",
      "Started BERTScore calculation\n",
      "Stated LLM as a Judge calculation\n",
      "Finished experiment for id: EA-17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triage_agent = Triage()\n",
    "execution_trail = \"\"\n",
    "experiment_results: list[ExperimentResult] = []\n",
    "\n",
    "for data_row in tqdm(random_data_set, desc=\"Running experiments\", unit=\"experiment\"):\n",
    "    result = ExperimentResult(\n",
    "        id=data_row.id,\n",
    "        question=data_row.question,\n",
    "        answer=data_row.answer,\n",
    "        product=data_row.product,\n",
    "        category=data_row.category,\n",
    "        persona=data_row.persona,\n",
    "        activity=data_row.activity,\n",
    "        country=data_row.country,\n",
    "    )\n",
    "    print(f\"Asking agent question with id: {data_row.id}\")\n",
    "    query_categorization = triage_agent.triage_user_message(\n",
    "        user_message=data_row.question\n",
    "    )\n",
    "    if DEBUG_MODE:\n",
    "        print(f\"Query categorization: {query_categorization}\")\n",
    "\n",
    "    tools = ToolsFabric.get_tools_for_category(\n",
    "        use_mcp=USE_MCP,\n",
    "        configuration=query_categorization[\"category\"],\n",
    "    )\n",
    "\n",
    "    if USE_MCP:\n",
    "        async with MultiServerMCPClient(tools) as client:\n",
    "            agent = ReActAgent(tool_list=client.get_tools())\n",
    "            execution_trail = await agent.arun_agent_with_input(\n",
    "                user_message=query_categorization[\"user_query\"], debug=DEBUG_MODE\n",
    "            )\n",
    "    else:\n",
    "        agent = ReActAgent(tool_list=tools)\n",
    "        execution_trail = agent.run_agent_with_input(\n",
    "            user_message=query_categorization[\"user_query\"], debug=DEBUG_MODE\n",
    "        )\n",
    "\n",
    "    run_data = agent.get_execution_data()\n",
    "\n",
    "    result.tools_used = run_data.tools_used\n",
    "    result.tool_calls_count = len(run_data.tools_used)\n",
    "    result.excecution_time_seconds = run_data.excecution_time_seconds\n",
    "    result.model_used = run_data.model_used\n",
    "    result.tokens_consumed = run_data.tokens_consumed\n",
    "    result.llm_call_count = run_data.llm_call_count\n",
    "    result.facts = data_row.facts\n",
    "    result.generated_answer = run_data.final_output\n",
    "\n",
    "    print(\"Finished agent execution\")\n",
    "\n",
    "    print(\"Started FactScore calculation\")\n",
    "    result.fact_score = await fact_scorer.get_fact_score(\n",
    "        facts=data_row.facts,\n",
    "        knowledge_source=result.generated_answer,\n",
    "        debug=DEBUG_MODE,\n",
    "    )\n",
    "\n",
    "    print(\"Started BERTScore calculation\")\n",
    "    result.bert_score = BertScore.compute_score(\n",
    "        expected_response=data_row.answer, actual_response=result.generated_answer\n",
    "    )\n",
    "\n",
    "    print(\"Stated LLM as a Judge calculation\")\n",
    "    result.llm_judge_model = LLM_JUDGE_MODEL\n",
    "    llm_evaluator = LLMAsJudgeEvaluator(model=LLM_JUDGE_MODEL)\n",
    "\n",
    "    result.llm_judge_outcome = llm_evaluator.evaluate(\n",
    "        question=data_row.question, generated_answer=result.generated_answer\n",
    "    )\n",
    "\n",
    "    print(f\"Finished experiment for id: {data_row.id}\\n\")\n",
    "\n",
    "    experiment_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i589287/Library/Caches/pypoetry/virtualenvs/react-agent-CUxtcVGL-py3.13/lib/python3.13/site-packages/pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `float` but got `FactScoreResult` with value `FactScoreResult(direct_fa...r_cloud~is_relevant.')])` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "product",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "persona",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "activity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "facts",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "bert_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "llm_judge_outcome",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_judge_model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "generated_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model_used",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tools_used",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tool_calls_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "excecution_time_seconds",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "llm_call_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fact_score.direct_fact_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fact_score.supporting_fact_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fact_score.combined_fact_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fact_score.direct_facts",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "fact_score.supporting_facts",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tokens_consumed.input_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tokens_consumed.output_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tokens_consumed.total_tokens",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "5570bde8-aabd-4f86-8966-db88f34badd2",
       "rows": [
        [
         "0",
         "EA-17",
         "Can you provide a sample implementation for IS_RELEVANT Method of BAdI EDCO_CLOUD_ADAPTOR to control eDocument creation in Public Cloud based on some conditions?",
         "Yes, I can provide a sample implementation for the IS_RELEVANT method of BAdI EDCO_CLOUD_ADAPTOR to control eDocument creation in Public Cloud based on some conditions.\nHere is an example implementation for the IS_RELEVANT method:\nCLASS zcl_edoc_adaptor_cloud DEFINITION\n  PUBLIC\n  FINAL\n  CREATE PUBLIC .\n  PUBLIC SECTION.\n    INTERFACES if_badi_interface .\n    INTERFACES if_edoc_adaptor_cloud .\n  PROTECTED SECTION.\n  PRIVATE SECTION.\nENDCLASS.\nCLASS zcl_badi_edoc_adaptor_cloud IMPLEMENTATION.\n  METHOD if_edoc_adaptor_cloud~is_relevant.\n    DATA: lv_relevant TYPE abap_bool.\n    \n    Your conditions to control eDocument creation\n    IF CONDITION.\n      lv_relevant = abap_true.\n    ELSE.\n      lv_relevant = abap_false.\n    ENDIF.\n    \n    rv_relevant = lv_relevant.\n  ENDMETHOD.\nENDCLASS.\nIn this implementation, you can define your own conditions to control eDocument creation based on your requirements. If the conditions are met, set the lv_relevant variable to abap_true, otherwise set it to abap_false. \nThe rv_relevant parameter is used to return the value of lv_relevant to the calling program.",
         null,
         null,
         null,
         null,
         null,
         "[{'fact': 'The IS_RELEVANT method is part of BAdI EDCO_CLOUD_ADAPTOR.', 'classification': 'direct', 'id': 'EA-17-1'}, {'fact': 'The IS_RELEVANT method controls eDocument creation in Public Cloud.', 'classification': 'direct', 'id': 'EA-17-2'}, {'fact': 'The implementation involves defining conditions to control eDocument creation.', 'classification': 'direct', 'id': 'EA-17-3'}, {'fact': 'The lv_relevant variable is set to abap_true if conditions are met.', 'classification': 'direct', 'id': 'EA-17-4'}, {'fact': 'The lv_relevant variable is set to abap_false if conditions are not met.', 'classification': 'direct', 'id': 'EA-17-5'}, {'fact': 'The rv_relevant parameter returns the value of lv_relevant to the calling program.', 'classification': 'direct', 'id': 'EA-17-6'}, {'fact': 'The sample implementation is written in ABAP.', 'classification': 'supporting', 'id': 'EA-17-7'}, {'fact': 'The implementation uses a class named zcl_badi_edoc_adaptor_cloud.', 'classification': 'supporting', 'id': 'EA-17-8'}, {'fact': 'The class zcl_badi_edoc_adaptor_cloud implements the method if_edoc_adaptor_cloud~is_relevant.', 'classification': 'supporting', 'id': 'EA-17-9'}]",
         "0.3681759238243103",
         "Helpful",
         "gpt-4o-mini",
         "Observation: The attempt to retrieve additional examples or guidelines using the `sap_help_lookup` tool resulted in a timeout error, preventing access to further information from SAP Help.\n\nThought: Given the timeout error, I will proceed with the information obtained from the `sap_documentation_summary` tool, which provides a detailed explanation of the implementation steps and logic for the IS_RELEVANT method.\n\nValidation Step:\n- The `sap_documentation_summary` tool provided a comprehensive explanation of the IS_RELEVANT method's implementation, including the logic for checking document relevance based on conditions like source type and billing header values.\n- Due to the timeout error with the `sap_help_lookup` tool, I am unable to cross-validate with additional examples or guidelines. However, the initial source is detailed enough to proceed with a sample implementation.\n\nFinal Answer:\nThe IS_RELEVANT method in the BAdI `EDOC_ADAPTOR_CLOUD` is implemented to control the creation of electronic documents based on specific conditions. Below is a sample implementation:\n\n```abap\nMETHOD if_edoc_adaptor_cloud~is_relevant.\n  DATA: lv_source_type TYPE string,\n        lv_netwr TYPE netwr.\n\n  lv_source_type = iv_source_type.\n  lv_netwr = billing_header-netwr.\n\n  IF lv_source_type = 'SD_INVOICE' AND lv_netwr = 0.\n    cv_relevant = abap_false.\n  ELSE.\n    cv_relevant = abap_true.\n  ENDIF.\nENDMETHOD.\n```\n\nExplanation:\n- The method checks if the `iv_source_type` is `'SD_INVOICE'` and if the `billing_header-netwr` is zero.\n- If both conditions are met, `cv_relevant` is set to `abap_false`, indicating the document is not relevant for electronic document creation.\n- Otherwise, `cv_relevant` remains `abap_true`, allowing document creation.\n\nThis implementation ensures that electronic documents are not created for zero-value invoices, aligning with typical business requirements. The method should be activated in the BAdI implementation to apply the logic effectively.\n\nTask complete.",
         "gpt-4o",
         "[{'tool_name': 'sap_documentation_summary', 'arguments': {'query': 'BAdI EDCO_CLOUD_ADAPTOR IS_RELEVANT method implementation'}}, {'tool_name': 'sap_help_lookup', 'arguments': {'query': 'EDCO_CLOUD_ADAPTOR IS_RELEVANT method'}}]",
         "2",
         "29.076",
         "0",
         "0.3333333333333333",
         "0.3333333333333333",
         "0.3333333333333333",
         "[{'fact': 'The IS_RELEVANT method is part of BAdI EDCO_CLOUD_ADAPTOR.', 'is_contained': False, 'reason': 'The context does not explicitly state that the IS_RELEVANT method is part of BAdI EDCO_CLOUD_ADAPTOR.'}, {'fact': 'The IS_RELEVANT method controls eDocument creation in Public Cloud.', 'is_contained': False, 'reason': \"The context does not explicitly state that the IS_RELEVANT method controls eDocument creation in Public Cloud. It describes the method's implementation logic but does not confirm its role in controlling eDocument creation in Public Cloud.\"}, {'fact': 'The implementation involves defining conditions to control eDocument creation.', 'is_contained': True, 'reason': 'The context provides a sample implementation of the IS_RELEVANT method, which includes conditions to control the creation of electronic documents based on specific criteria.'}, {'fact': 'The lv_relevant variable is set to abap_true if conditions are met.', 'is_contained': True, 'reason': 'The context provides a sample implementation where cv_relevant is set to abap_true if the conditions are not met, implying it is set to abap_true when conditions are met.'}, {'fact': 'The lv_relevant variable is set to abap_false if conditions are not met.', 'is_contained': False, 'reason': 'The context does not explicitly mention the lv_relevant variable or its conditions. It discusses the cv_relevant variable and its conditions, but not lv_relevant.'}, {'fact': 'The rv_relevant parameter returns the value of lv_relevant to the calling program.', 'is_contained': False, 'reason': 'The context does not mention the rv_relevant parameter or its behavior in relation to lv_relevant.'}]",
         "[{'fact': 'The sample implementation is written in ABAP.', 'is_contained': True, 'reason': 'The context provides a sample implementation of the IS_RELEVANT method written in ABAP, confirming the fact.'}, {'fact': 'The implementation uses a class named zcl_badi_edoc_adaptor_cloud.', 'is_contained': False, 'reason': 'The context does not mention the use of a class named zcl_badi_edoc_adaptor_cloud.'}, {'fact': 'The class zcl_badi_edoc_adaptor_cloud implements the method if_edoc_adaptor_cloud~is_relevant.', 'is_contained': False, 'reason': 'The context does not explicitly state that the class zcl_badi_edoc_adaptor_cloud implements the method if_edoc_adaptor_cloud~is_relevant.'}]",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 26,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>product</th>\n",
       "      <th>category</th>\n",
       "      <th>persona</th>\n",
       "      <th>activity</th>\n",
       "      <th>country</th>\n",
       "      <th>facts</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>...</th>\n",
       "      <th>excecution_time_seconds</th>\n",
       "      <th>llm_call_count</th>\n",
       "      <th>fact_score.direct_fact_score</th>\n",
       "      <th>fact_score.supporting_fact_score</th>\n",
       "      <th>fact_score.combined_fact_score</th>\n",
       "      <th>fact_score.direct_facts</th>\n",
       "      <th>fact_score.supporting_facts</th>\n",
       "      <th>tokens_consumed.input_tokens</th>\n",
       "      <th>tokens_consumed.output_tokens</th>\n",
       "      <th>tokens_consumed.total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EA-17</td>\n",
       "      <td>Can you provide a sample implementation for IS...</td>\n",
       "      <td>Yes, I can provide a sample implementation for...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'fact': 'The IS_RELEVANT method is part of B...</td>\n",
       "      <td>0.368176</td>\n",
       "      <td>...</td>\n",
       "      <td>29.076</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[{'fact': 'The IS_RELEVANT method is part of B...</td>\n",
       "      <td>[{'fact': 'The sample implementation is writte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question  \\\n",
       "0  EA-17  Can you provide a sample implementation for IS...   \n",
       "\n",
       "                                              answer product category persona  \\\n",
       "0  Yes, I can provide a sample implementation for...    None     None    None   \n",
       "\n",
       "  activity country                                              facts  \\\n",
       "0     None    None  [{'fact': 'The IS_RELEVANT method is part of B...   \n",
       "\n",
       "   bert_score  ... excecution_time_seconds llm_call_count  \\\n",
       "0    0.368176  ...                  29.076              0   \n",
       "\n",
       "  fact_score.direct_fact_score fact_score.supporting_fact_score  \\\n",
       "0                     0.333333                         0.333333   \n",
       "\n",
       "  fact_score.combined_fact_score  \\\n",
       "0                       0.333333   \n",
       "\n",
       "                             fact_score.direct_facts  \\\n",
       "0  [{'fact': 'The IS_RELEVANT method is part of B...   \n",
       "\n",
       "                         fact_score.supporting_facts  \\\n",
       "0  [{'fact': 'The sample implementation is writte...   \n",
       "\n",
       "   tokens_consumed.input_tokens  tokens_consumed.output_tokens  \\\n",
       "0                             0                              0   \n",
       "\n",
       "   tokens_consumed.total_tokens  \n",
       "0                             0  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = [r.model_dump(mode=\"json\") for r in experiment_results]\n",
    "df = pd.json_normalize(records)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"data.json\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "react-agent-CUxtcVGL-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
